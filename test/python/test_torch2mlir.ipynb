{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test torch2mlir converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. JIT Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module {\n",
      "  func.func @main(%arg0: !torch.vtensor<[4,3],f32>, %arg1: !torch.vtensor<[4],f32>, %arg2: !torch.vtensor<[3],f32>) -> (!torch.vtensor<[3],f32>, !torch.vtensor<[1,4],f32>) {\n",
      "    %int0 = torch.constant.int 0\n",
      "    %0 = torch.aten.unsqueeze %arg1, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>\n",
      "    %1 = torch.aten.mm %0, %arg0 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>\n",
      "    %int0_0 = torch.constant.int 0\n",
      "    %2 = torch.aten.squeeze.dim %1, %int0_0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>\n",
      "    %int1 = torch.constant.int 1\n",
      "    %3 = torch.aten.add.Tensor %2, %arg2, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>\n",
      "    return %3, %0 : !torch.vtensor<[3],f32>, !torch.vtensor<[1,4],f32>\n",
      "  }\n",
      "}\n",
      "\n",
      "#map = affine_map<(d0) -> (d0)>\n",
      "module {\n",
      "  util.func public @main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.fence, %arg4: !hal.fence) -> (!hal.buffer_view, !hal.buffer_view) attributes {inlining_policy = #util.inline.never, iree.abi.model = \"coarse-fences\", iree.abi.stub} {\n",
      "    %cst = arith.constant 0.000000e+00 : f32\n",
      "    %0 = hal.tensor.import wait(%arg3) => %arg0 : !hal.buffer_view -> tensor<4x3xf32>\n",
      "    %1 = hal.tensor.import wait(%arg3) => %arg1 : !hal.buffer_view -> tensor<4xf32>\n",
      "    %2 = hal.tensor.import wait(%arg3) => %arg2 : !hal.buffer_view -> tensor<3xf32>\n",
      "    %expanded = tensor.expand_shape %1 [[0, 1]] output_shape [1, 4] : tensor<4xf32> into tensor<1x4xf32>\n",
      "    %3 = tensor.empty() : tensor<1x3xf32>\n",
      "    %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>\n",
      "    %5 = linalg.matmul ins(%expanded, %0 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>\n",
      "    %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>\n",
      "    %6 = tensor.empty() : tensor<3xf32>\n",
      "    %7 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%collapsed, %2 : tensor<3xf32>, tensor<3xf32>) outs(%6 : tensor<3xf32>) {\n",
      "    ^bb0(%in: f32, %in_0: f32, %out: f32):\n",
      "      %11 = arith.addf %in, %in_0 : f32\n",
      "      linalg.yield %11 : f32\n",
      "    } -> tensor<3xf32>\n",
      "    %8:2 = hal.tensor.barrier join(%7, %expanded : tensor<3xf32>, tensor<1x4xf32>) => %arg4 : !hal.fence\n",
      "    %9 = hal.tensor.export %8#0 : tensor<3xf32> -> !hal.buffer_view\n",
      "    %10 = hal.tensor.export %8#1 : tensor<1x4xf32> -> !hal.buffer_view\n",
      "    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view\n",
      "  }\n",
      "  util.func public @main(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub} {\n",
      "    %0 = util.null : !hal.fence\n",
      "    %c-1_i32 = arith.constant -1 : i32\n",
      "    %c0 = arith.constant 0 : index\n",
      "    %device_0 = hal.devices.get %c0 : !hal.device\n",
      "    %fence = hal.fence.create device(%device_0 : !hal.device) flags(\"None\") : !hal.fence\n",
      "    %1:2 = util.call @main$async(%arg0, %arg1, %arg2, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> (!hal.buffer_view, !hal.buffer_view)\n",
      "    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags(\"None\") : i32\n",
      "    util.return %1#0, %1#1 : !hal.buffer_view, !hal.buffer_view\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the `nn.Module` or Python function to run.\n",
    "class LinearModule(torch.nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "    self.weight = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  def forward(self, input):\n",
    "    return (input @ self.weight) + self.bias\n",
    "\n",
    "linear_module = LinearModule(4, 3)\n",
    "\n",
    "# Compile the program using the turbine backend.\n",
    "opt_linear_module = torch.compile(linear_module, backend=\"turbine_cpu\")\n",
    "\n",
    "# Use the compiled program as you would the original program.\n",
    "args = torch.randn(4)\n",
    "turbine_output = opt_linear_module(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AOT Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5868226  4.543955   6.291369 ]\n"
     ]
    }
   ],
   "source": [
    "import iree.runtime as ireert\n",
    "import numpy as np\n",
    "import iree.turbine.aot as aot\n",
    "import torch\n",
    "\n",
    "# Define the `nn.Module` to export.\n",
    "class LinearModule(torch.nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "    self.weight = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  def forward(self, input):\n",
    "    return (input @ self.weight) + self.bias\n",
    "\n",
    "linear_module = LinearModule(4, 3)\n",
    "\n",
    "# Export the program using the simple API.\n",
    "example_arg = torch.randn(4)\n",
    "export_output = aot.export(linear_module, example_arg)\n",
    "\n",
    "export_output.save_mlir(file_path='./linearmodule.mlir')\n",
    "\n",
    "# Compile to a deployable artifact.\n",
    "binary = export_output.compile(save_to=None)\n",
    "binary_save = export_output.compile(save_to='binary_saved.bin')\n",
    "\n",
    "# Use the IREE runtime API to test the compiled program.\n",
    "config = ireert.Config(\"local-task\")\n",
    "vm_module = ireert.load_vm_module(\n",
    "    ireert.VmModule.copy_buffer(config.vm_instance, binary.map_memory()),\n",
    "    config,\n",
    ")\n",
    "\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "result = vm_module.main(input)\n",
    "print(result.to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AOT Exporting with external params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "import numpy as np\n",
    "import iree.turbine.aot as aot\n",
    "\n",
    "class LinearModule(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input @ self.weight) + self.bias\n",
    "\n",
    "linear_module = LinearModule(4,3)\n",
    "\n",
    "# Create a params dictionary. Note that the keys here match LinearModule's\n",
    "# attributes. We will use the saved safetensor file for use from the command\n",
    "# line.\n",
    "wt = linear_module.weight.data.contiguous()\n",
    "bias = linear_module.bias.data.contiguous()\n",
    "params = { \"weight\": wt, \"bias\": bias }\n",
    "save_file(params, \"params.safetensors\")\n",
    "\n",
    "# Externalize the model parameters. This removes weight tensors from the IR\n",
    "# module, allowing them to be loaded at runtime. Symbolic references to these\n",
    "# parameters are still retained in the IR.\n",
    "aot.externalize_module_parameters(linear_module)\n",
    "\n",
    "input = torch.randn(4)\n",
    "exported_module = aot.export(linear_module, input)\n",
    "exported_module.save_mlir(file_path='./linearmodule_external_params.mlir')\n",
    "\n",
    "# Compile the exported module, to generate the binary. When `save_to` is\n",
    "# not None, the binary will be stored at the path passed in to `save_to`.\n",
    "# Here, we pass in None, so that the binary can stored in a variable.\n",
    "binary = exported_module.compile(save_to=None)\n",
    "binary_saved = exported_module.compile(save_to='binary_external_params.bin')\n",
    "\n",
    "# Save the input as an npy tensor, so that it can be passed in through the\n",
    "# command line to `iree-run-module`.\n",
    "input_np = input.numpy()\n",
    "np.save(\"input.npy\", input_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
